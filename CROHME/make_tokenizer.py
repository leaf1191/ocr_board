from tokenizers import Tokenizer

# ì €ì¥ëœ tokenizer ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer_path = "C:/Users/SS/PycharmProjects/CvTermproject/CROHME/tokenizerfix.json"
tokenizer = Tokenizer.from_file(tokenizer_path)

# ğŸ” 1. íŠ¹ì • í† í°ì´ vocabì— ìˆëŠ”ì§€ í™•ì¸
def check_token(token):
    try:
        token_id = tokenizer.token_to_id(token)
        if token_id is not None:
            print(f"âœ… í† í° '{token}' ì€ vocabì— ìˆìŒ (ID: {token_id})")
        else:
            print(f"âŒ í† í° '{token}' ì€ vocabì— ì—†ìŒ")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

check_token("v")
check_token("(")
check_token("x")
check_token("v(x)")  # ê³µë°± ì—†ì´ë„ ì²´í¬í•´ë³´ì
check_token("[UNK]")  # í•­ìƒ í¬í•¨ë˜ì–´ì•¼ í•¨

# ğŸ“‹ 2. ì „ì²´ vocab ì¼ë¶€ ì¶œë ¥ (ì˜ˆ: ì²˜ìŒ 20ê°œ)
print("\nğŸ“š vocab ì¼ë¶€:")
vocab = tokenizer.get_vocab()
sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])  # ID ê¸°ì¤€ ì •ë ¬
for token, token_id in sorted_vocab[:20]:
    print(f"{token_id}: {token}")
